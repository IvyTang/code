# model configuration
init_scale 0.05
learning_rate 0.2
max_grad_norm 5
num_layers 2
num_steps 30
hidden_size 250
max_epoch 2
keep_prob 0.75
lr_decay 0.8
batch_size 128
vocab_size 20000
vocab_size_in 20000
vocab_size_out 20000
max_max_epoch 10
basis_size 2000
sparsity 10
finetune_learning_rate 0.25
finetune_epoch 10
gpu_fraction 0.90
buckets 5,10,20,30
